{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [作業重點]\n",
    "目前你應該已經要很清楚資料集中，資料的型態是什麼樣子囉！包含特徵 (features) 與標籤 (labels)。因此要記得未來不管什麼專案，必須要把資料清理成相同的格式，才能送進模型訓練。\n",
    "今天的作業開始踏入決策樹這個非常重要的模型，請務必確保你理解模型中每個超參數的意思，並試著調整看看，對最終預測結果的影響為何"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業\n",
    "\n",
    "1. 試著調整 DecisionTreeClassifier(...) 中的參數，並觀察是否會改變結果？\n",
    "2. 改用其他資料集 (boston, wine)，並與回歸模型的結果進行比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuuracy_train:  1.0\n",
      "Acuuracy_test:  0.9736842105263158\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Feature importance:  [0.01796599 0.         0.05992368 0.92211033]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets, metrics\n",
    "\n",
    "# 如果是分類問題，請使用 DecisionTreeClassifier，若為回歸問題，請使用 DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 讀取鳶尾花資料集\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# 切分訓練集/測試集\n",
    "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25, random_state=4)\n",
    "\n",
    "# 建立模型\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# 訓練模型\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# 預測測試集\n",
    "y_pred_test = clf.predict(x_test)\n",
    "y_pred_train = clf.predict(x_train)\n",
    "acc_train = metrics.accuracy_score(y_train, y_pred_train)\n",
    "print(\"Acuuracy_train: \", acc_train)\n",
    "\n",
    "acc_test = metrics.accuracy_score(y_test, y_pred_test)\n",
    "print(\"Acuuracy_test: \", acc_test)\n",
    "\n",
    "print(iris.feature_names)\n",
    "print(\"Feature importance: \", clf.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuuracy_train:  0.9821428571428571\n",
      "Acuuracy_test:  0.9736842105263158\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Feature importance:  [0.         0.         0.08469539 0.91530461]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bobson\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\tree\\tree.py:282: DeprecationWarning: The min_impurity_split parameter is deprecated and will be removed in version 0.21. Use the min_impurity_decrease parameter instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "################# 作業 1 #######################################\n",
    "### 目前 default train acc : 1, test_acc = 0.9736 , 其實已經相當準確, 看看是否能再提高 test_acc\n",
    "### a.試著讓 training 不要學過多:\n",
    "# 1. 如果減少 max_depth 會減少 Model overfitting, 但如果減少到2以下會 underfitting, 目前看來設定為 3 較好  \n",
    "# 2. 如果增加 min_samples_split, min_samples_leaf, min_impurity_decrease, min_impurity_split 也可以減少 overtting 的情況\n",
    "# ==> 但僅都降低 train acc, 無法提升 test acc\n",
    "### b.試著調整class_weight, 以處理類別資料量不平衡的情況\n",
    "# ==> class_weight 調整為 balanced 以處理類別資料量不平衡的情況 ==> 僅增加 training 準確率 \n",
    "\n",
    "##################################################################\n",
    "from sklearn import datasets, metrics\n",
    "\n",
    "# 如果是分類問題，請使用 DecisionTreeClassifier，若為回歸問題，請使用 DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 讀取鳶尾花資料集\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# 切分訓練集/測試集\n",
    "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25, random_state=4)\n",
    "\n",
    "# 建立模型\n",
    "################################### 參數設定 ###################################\n",
    "##### 改變 Criterion #####\n",
    "criterion = 'gini'\n",
    "\n",
    "### 關於切割設定, 可加快速度/減少overfitting ###\n",
    "splitter = 'best'\n",
    "max_features = 0.75 \n",
    "min_samples_split = 5\n",
    "min_samples_leaf = 3\n",
    "min_impurity_decrease = 0.001\n",
    "min_impurity_split = 1e-7\n",
    "min_weight_fraction_leaf = 0\n",
    "### 關於模型架構 ###\n",
    "max_depth = 3\n",
    "max_leaf_nodes = None\n",
    "\n",
    "### Others ###\n",
    "class_weight = 'balanced'\n",
    "presort = False\n",
    "random_state = 0\n",
    "\n",
    "##################################################################################\n",
    "clf = DecisionTreeClassifier(criterion = criterion, min_samples_leaf=min_samples_leaf, max_depth = max_depth, max_features = max_features, min_samples_split = min_samples_split, min_impurity_decrease = min_impurity_decrease, min_impurity_split = min_impurity_split, class_weight =class_weight, presort = presort, random_state = random_state)\n",
    "\n",
    "# 訓練模型\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# 預測測試集\n",
    "y_pred_test = clf.predict(x_test)\n",
    "y_pred_train = clf.predict(x_train)\n",
    "acc_train = metrics.accuracy_score(y_train, y_pred_train)\n",
    "print(\"Acuuracy_train: \", acc_train)\n",
    "\n",
    "acc_test = metrics.accuracy_score(y_test, y_pred_test)\n",
    "print(\"Acuuracy_test: \", acc_test)\n",
    "\n",
    "print(iris.feature_names)\n",
    "print(\"Feature importance: \", clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 13) (178,)\n",
      "Acuuracy_train_1:  0.6901408450704225\n",
      "Acuuracy_test_1:  0.75\n",
      "Acuuracy_train:  0.9788732394366197\n",
      "Acuuracy_test:  0.9166666666666666\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Feature importance:  [0.         0.00818213 0.         0.         0.         0.\n",
      " 0.00487901 0.         0.03219561 0.12169062 0.         0.42291308\n",
      " 0.41013955]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bobson\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Bobson\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\tree\\tree.py:282: DeprecationWarning: The min_impurity_split parameter is deprecated and will be removed in version 0.21. Use the min_impurity_decrease parameter instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "############## 作業 2 ###############################\n",
    "### 在 Wine 這比資料中 Tree 的 結果明顯比 Logistic Regression 好很多, logistic Regression 仍為 Under Fitting\n",
    "### tree 的 train和 test acc皆已超過 0.9 \n",
    "### Logistic Regression 仍需做 Feature selection 或其他參數調整才能再進一步提升準確率\n",
    "\n",
    "###########################################################\n",
    "##########   wine     ###################\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "wine = datasets.load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "print(X.shape, y.shape)\n",
    "### split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)\n",
    "\n",
    "############## Linear Model ######################\n",
    "#########################################\n",
    "### find significance features\n",
    "\n",
    "model1 = LogisticRegression(solver = 'saga', multi_class='multinomial')\n",
    "\n",
    "model1.fit(X_train, y_train)\n",
    "# 預測測試集\n",
    "y_pred_test = model1.predict(X_test)\n",
    "y_pred_train = model1.predict(X_train)\n",
    "acc_train = metrics.accuracy_score(y_train, y_pred_train)\n",
    "print(\"Acuuracy_train_1: \", acc_train)\n",
    "\n",
    "acc_test = metrics.accuracy_score(y_test, y_pred_test)\n",
    "print(\"Acuuracy_test_1: \", acc_test)\n",
    "\n",
    "############  Tree #####################################################################\n",
    "################################### 參數設定 ###################################\n",
    "##### 改變 Criterion #####\n",
    "criterion = 'gini'\n",
    "\n",
    "### 關於切割設定, 可加快速度/減少overfitting ###\n",
    "splitter = 'best'\n",
    "max_features = 0.75 \n",
    "min_samples_split = 5\n",
    "min_samples_leaf = 3\n",
    "min_impurity_decrease = 0.001\n",
    "min_impurity_split = 1e-7\n",
    "min_weight_fraction_leaf = 0\n",
    "### 關於模型架構 ###\n",
    "max_depth = 6\n",
    "max_leaf_nodes = None\n",
    "\n",
    "### Others ###\n",
    "class_weight = 'balanced'\n",
    "presort = False\n",
    "random_state = 0\n",
    "\n",
    "tree1 = DecisionTreeClassifier(criterion = criterion, min_samples_leaf=min_samples_leaf, max_depth = max_depth, max_features = max_features, min_samples_split = min_samples_split, min_impurity_decrease = min_impurity_decrease, min_impurity_split = min_impurity_split, class_weight =class_weight, presort = presort, random_state = random_state)\n",
    "tree1.fit(X_train, y_train)\n",
    "\n",
    "# 預測測試集\n",
    "y_pred_test = tree1.predict(X_test)\n",
    "y_pred_train = tree1.predict(X_train)\n",
    "acc_train = metrics.accuracy_score(y_train, y_pred_train)\n",
    "print(\"Acuuracy_train: \", acc_train)\n",
    "\n",
    "acc_test = metrics.accuracy_score(y_test, y_pred_test)\n",
    "print(\"Acuuracy_test: \", acc_test)\n",
    "\n",
    "print(iris.feature_names)\n",
    "print(\"Feature importance: \", tree1.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13) (506,)\n",
      "training R2: 0.7729718726571158\n",
      "testing R2: 0.5892011519186435\n",
      "training R2: 0.8592673987932495\n",
      "testing R2: 0.6175234803465643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bobson\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\tree\\tree.py:282: DeprecationWarning: The min_impurity_split parameter is deprecated and will be removed in version 0.21. Use the min_impurity_decrease parameter instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#################### 作業 2 ##########################################\n",
    "### 在 Boston 這比資料中 2 方法 testing r2 差不多, 但是 tree 很明顯有嚴重 overfitting, 需要進行參數調整以減少 overfitting 現象\n",
    "### 而 regression model 為 under fitting 看來需要增加有意義(非線性關係) feature 以提升 r2\n",
    "\n",
    "##########   boston     ###################\n",
    "#########################################\n",
    "#####################################\n",
    "boston = datasets.load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "### split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2 , random_state = 0)\n",
    "################# Linear Model ##########################################\n",
    "model2 = LinearRegression()\n",
    "model2.fit(X_train, y_train)\n",
    "y_pred_test = model2.predict(X_test)\n",
    "y_pred_train = model2.predict(X_train)\n",
    "print('training R2:', model2.score(X_train, y_train))\n",
    "print('testing R2:', model2.score(X_test, y_test))\n",
    "\n",
    "############### Tree ########################\n",
    "##### 改變 Criterion #####\n",
    "criterion = 'mse'\n",
    "\n",
    "### 關於切割設定, 可加快速度/減少overfitting ###\n",
    "splitter = 'best'\n",
    "max_features = None \n",
    "min_samples_split = 25\n",
    "min_samples_leaf = 10\n",
    "min_impurity_decrease = 0\n",
    "min_impurity_split = 0\n",
    "min_weight_fraction_leaf = 0\n",
    "### 關於模型架構 ###\n",
    "max_depth = 10\n",
    "max_leaf_nodes = 15\n",
    "\n",
    "### Others ###\n",
    "presort = False\n",
    "random_state = 0\n",
    "tree2 = DecisionTreeRegressor(criterion = criterion, min_samples_leaf=min_samples_leaf, max_depth = max_depth, max_features = max_features, min_samples_split = min_samples_split, min_impurity_decrease = min_impurity_decrease, min_impurity_split = min_impurity_split, presort = presort, random_state = random_state, max_leaf_nodes = max_leaf_nodes)\n",
    "tree2.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = tree2.predict(X_test)\n",
    "y_pred_train = tree2.predict(X_train)\n",
    "print('training R2:', tree2.score(X_train, y_train))\n",
    "print('testing R2:', tree2.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
